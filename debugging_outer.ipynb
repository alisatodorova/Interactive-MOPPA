{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import momepy\n",
    "import networkx as nx\n",
    "# import pandas as pd\n",
    "# import shapely\n",
    "# import shapely.geometry as sg\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from lmzintgraf_gp_pref_elicit import dataset, gaussian_process, acquisition_function\n",
    "from lmzintgraf_gp_pref_elicit.gp_utilities import utils_ccs as utils_ccs\n",
    "from lmzintgraf_gp_pref_elicit.gp_utilities import utils_data as utils_data\n",
    "from lmzintgraf_gp_pref_elicit.gp_utilities import utils_experiment as utils_experiment\n",
    "from lmzintgraf_gp_pref_elicit.gp_utilities import utils_parameters as utils_parameters\n",
    "from lmzintgraf_gp_pref_elicit.gp_utilities import utils_user as utils_user"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "map = gpd.read_file(\"Sidewalk_width_crossings.geojson\") #smaller ~180 nodes\n",
    "\n",
    "# Objectives\n",
    "objective1 = map['length']\n",
    "# print(objective1)\n",
    "objective2 = map['crossing']\n",
    "objective3 = map['obstacle_free_width']\n",
    "print(objective3.unique())\n",
    "# filtered_map = map.loc[map['obstacle_free_width'] == '>2.9m']\n",
    "# # for value in objective3:\n",
    "# #     width = value\n",
    "# #     if width == '>2.9m':\n",
    "# #         objective = value\n",
    "# for index, row in filtered_map.iterrows():\n",
    "#     # Access the properties of each filtered node\n",
    "#     node_id = row['id']\n",
    "#     # Perform operations on the filtered nodes\n",
    "#     # print(f\"Node ID: {node_id}\")\n",
    "#     # print(\"Properties:\")\n",
    "#     print(row)\n",
    "objectives = ('length', 'crossing')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a NetworkX graph from the map\n",
    "G = momepy.gdf_to_nx(map, approach='primal')\n",
    "nodes = G.nodes\n",
    "# print(len(nodes))\n",
    "edges = G.edges\n",
    "# print(len(max(nx.connected_components(G), key=len)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #Show the map as nodes and edges\n",
    "#\n",
    "# from shapely.geometry import Point\n",
    "# start_node = gpd.GeoDataFrame({'geometry': [Point(122245.37633330293,\n",
    "#                                                   486126.8581684635)]})\n",
    "# end_node = gpd.GeoDataFrame({'geometry': [Point(122320.31466476223, 486327.5294561802)]})\n",
    "# fig, ax = plt.subplots(figsize=(14,14), dpi=600)\n",
    "# # All nodes and edges\n",
    "# nx.draw(G, {n:[n[0], n[1]] for n in list(G.nodes)}, ax=ax, node_size=3)\n",
    "# # Start & end node\n",
    "# start_node.plot(ax=ax, color='red')\n",
    "# end_node.plot(ax=ax, color='purple')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(nodes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Pick random ones or pick manually that make sense - to experiment\n",
    "#Smaller map:\n",
    "# S = (122245.37633330293, 486126.8581684635)\n",
    "# T = (122384.20250442973, 486270.65737816785) #AxisError\n",
    "# T = (122320.31466476223, 486327.5294561802) #t !=cost #output2.png\n",
    "# T = (122246.77932030056, 486223.5791244763) #t = cost; output.png\n",
    "S = (122245.37633330293, 486126.8581684635)\n",
    "T = (122246.77932030056, 486223.5791244763)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import outer_loop\n",
    "t, p_star, val_vector_p_star, P = outer_loop.outer(G, S, T, objectives)\n",
    "print(f\"Target {t}; Path {p_star} with cost {val_vector_p_star}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot experiments\n",
    "\n",
    "from shapely.geometry import Point\n",
    "start_node = gpd.GeoDataFrame({'geometry': [Point(122245.37633330293,\n",
    "                                                  486126.8581684635)]})\n",
    "end_node = gpd.GeoDataFrame({'geometry': [Point(122320.31466476223, 486327.5294561802)]})\n",
    "fig, ax = plt.subplots(figsize=(14,14), dpi=600)\n",
    "# All nodes and edges\n",
    "nx.draw(G, {n:[n[0], n[1]] for n in list(G.nodes)}, ax=ax, node_size=3)\n",
    "# Start & end node\n",
    "start_node.plot(ax=ax, color='red')\n",
    "end_node.plot(ax=ax, color='purple')\n",
    "\n",
    "# Path p_star\n",
    "path_edges = list(zip(p_star[:-1], p_star[1:]))\n",
    "nx.draw_networkx_edges(G, pos={n: [n[0], n[1]] for n in list(G.nodes)}, edgelist=path_edges, ax=ax, edge_color='green', width=2)\n",
    "\n",
    "\n",
    "\n",
    "# ax.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialise the Gaussian process for 2 objectives\n",
    "gp = gaussian_process.GPPairwise(num_objectives=2, std_noise=0.01, kernel_width=0.15, prior_mean_type='zero', seed=None)\n",
    "\n",
    "P = []  # Pareto set\n",
    "val_p = []  # value vectors w.r.t. p, i.e., v^{p_1}, v^{p_2}\n",
    "val_vector_p_star = []  # value vectors w.r.t. p^*\n",
    "compare_ps_pstar = []\n",
    "\n",
    "# Path initialisation\n",
    "for i in objectives:\n",
    "    p = nx.shortest_path(G, source=S, target=T, weight=i, method='dijkstra')  # Dijkstra's algorithm\n",
    "    P.append(p)\n",
    "\n",
    "    # Computes the total cost associated with the path and objective, i.e., the value of the path\n",
    "    val_obj1 = nx.path_weight(G, path=p, weight='length')\n",
    "    val_obj2 = nx.path_weight(G, path=p, weight='crossing')\n",
    "    val_p.append(np.array([val_obj1, val_obj2]))\n",
    "\n",
    "# Candidate Targets, i.e., the most optimistic points\n",
    "C = [min(val_p[0][0], val_p[1][0]), min(val_p[0][1], val_p[1][1])]\n",
    "C = [np.array(C)]\n",
    "\n",
    "# The most pessimistic points form the upper bounds\n",
    "U = [max(val_p[0][0], val_p[1][0]), max(val_p[0][1], val_p[1][1])]\n",
    "print(U)\n",
    "\n",
    "# User ranking: Compare paths in P\n",
    "user_preference = utils_user.UserPreference(num_objectives=2, std_noise=0.1)\n",
    "add_noise = True\n",
    "ground_utility = user_preference.get_preference(val_p, add_noise=add_noise)  # This is the ground-truth utility, i.e., the true utility\n",
    "\n",
    "# Add the comparisons to the GP\n",
    "comparisons = dataset.DatasetPairwise(num_objectives=2)\n",
    "comparisons.add_single_comparison(val_p[np.argmax(ground_utility)], val_p[np.argmin(ground_utility)])  # This is user ranking of their preferences\n",
    "gp.update(comparisons)\n",
    "\n",
    "# Find the path the user likes best and has the maximum a posteriori (MAP) estimate\n",
    "u_v, _ = gp.get_predictive_params(val_p, True)  # The maximum a posteriori (MAP) estimate is the mean from gaussian_process.get_predictive_params()\n",
    "p_star_index = np.argmax(u_v)\n",
    "p_star = P[p_star_index]\n",
    "\n",
    "# Computes the total cost associated with the path and objective, i.e., the value of the path\n",
    "val_p_star1 = nx.path_weight(G, path=p_star, weight='length')\n",
    "val_p_star2 = nx.path_weight(G, path=p_star, weight='crossing')\n",
    "val_vector_p_star.append(np.array([val_p_star1, val_p_star2]))\n",
    "\n",
    "# Initialise the acquisition function\n",
    "input_domain = np.array(C)  # set of Candidate targets\n",
    "acq_fun = acquisition_function.DiscreteAcquirer(input_domain=input_domain, query_type='ranking', seed=123, acquisition_type='expected improvement')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import dfs_lower\n",
    "while len(C) != 0:\n",
    "    # Pick the Candidate target which has the highest value from the acquisition function\n",
    "    expected_improvement = acquisition_function.get_expected_improvement(input_domain, gp, acq_fun.history)\n",
    "    t_index = np.argmax(expected_improvement)\n",
    "    t = input_domain[t_index]\n",
    "#\n",
    "#     # Remove t from C\n",
    "#     C = np.delete(C, np.where(np.all(C == t)))\n",
    "    indices = [i for i, x in enumerate(C) if np.all(x == t)]\n",
    "    for index in sorted(indices, reverse=True):\n",
    "        del C[index]\n",
    "\n",
    "    # Inner-loop approach with DFS guided by the lower-bounds computed from the single-objective value iteration\n",
    "    p_s, val_p_s, new_U = dfs_lower.dfs_lower(G, S, T, t, U, max_iter=1000)\n",
    "    U = new_U\n",
    "\n",
    "    # If v^p_s improves in the target region\n",
    "    if np.any(np.greater(val_p_s, U)):\n",
    "        P = P.append(p_s)\n",
    "\n",
    "        # Compare p^s to p^∗ and add comparison to the GP ▷ User ranking, i.e., is the new path preferred to the current, maximum one?\n",
    "        compare_ps_pstar.append(np.array([val_p_s, val_vector_p_star]))\n",
    "        ranking_new_paths = user_preference.get_preference(compare_ps_pstar, add_noise=add_noise)\n",
    "\n",
    "        # Add the comparisons to the GP\n",
    "        comparisons.add_single_comparison(compare_ps_pstar[np.argmax(ranking_new_paths)], compare_ps_pstar[np.argmin(ranking_new_paths)])\n",
    "        gp.update(comparisons)\n",
    "\n",
    "        # if u(v^{p^s}) > u(v^{p^*}) then\n",
    "        u_v_p_s, _ = gp.get_predictive_params(val_p_s, True)  # The maximum a posteriori (MAP) estimate is the mean from gaussian_process.get_predictive_params()\n",
    "        u_v_p_star, _ = gp.get_predictive_params(val_vector_p_star, True)\n",
    "\n",
    "        if u_v_p_s > u_v_p_star:\n",
    "            # p^∗ ← p^s\n",
    "            p_star = p_s\n",
    "\n",
    "        # Compute new candidate targets based on v^{p^s} and add to C\n",
    "        new_C = [min(val_p_s[0][0], val_p_s[1][0]), min(val_p_s[0][1], val_p_s[1][1])]\n",
    "        C = np.append(new_C)\n",
    "\n",
    "print(f\"Target {t}; Path {p_star} with cost {val_vector_p_star}\")\n",
    "# print(\"Stop\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialise the Gaussian process for 2 objectives\n",
    "gp = gaussian_process.GPPairwise(num_objectives=2, std_noise=0.01, kernel_width=0.15,prior_mean_type='zero', seed=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "P = [] #Pareto set\n",
    "p = [] #paths computed by Dijkstra's algorithm\n",
    "val_vector_p = [] #value vectors w.r.t. p, i.e., v^{p_1}, v^{p_2}\n",
    "\n",
    "# Path initialisation\n",
    "for i in objectives:\n",
    "    p = nx.shortest_path(G, source=S, target=T, weight=i, method='dijkstra') #Dijkstra's algorithm\n",
    "    P.append(p)\n",
    "\n",
    "    val_obj1 = nx.path_weight(G, path=p, weight='length') #Returns total cost associated with the path and weight. In other words, it returns the value of the path.\n",
    "    val_obj2 = nx.path_weight(G, path=p, weight='crossing')\n",
    "    val_vector_p.append(np.array([val_obj1, val_obj2]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_vector_p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Candidate Targets, i.e., the most optimistic points\n",
    "C = [min(val_vector_p[0][0], val_vector_p[1][0]), min(val_vector_p[0][1], val_vector_p[1][1])]\n",
    "C = [np.array(C)]\n",
    "print(C)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The most pessimistic points form the upper bounds\n",
    "U = [max(val_vector_p[0][0], val_vector_p[1][0]), max(val_vector_p[0][1], val_vector_p[1][1])]\n",
    "U"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# User ranking: Compare paths in P\n",
    "user_preference = utils_user.UserPreference(num_objectives=2, std_noise=0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "add_noise = True\n",
    "ground_utility = user_preference.get_preference(val_vector_p, add_noise=add_noise) #This is the ground-truth utility, i.e., the true utility\n",
    "print(ground_utility)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add the comparisons to GP\n",
    "comparisons = dataset.DatasetPairwise(num_objectives=2)\n",
    "\n",
    "comparisons.add_single_comparison(val_vector_p[np.argmax(ground_utility)], val_vector_p[np.argmin(ground_utility)]) #This way we are performing user ranking of their preferences\n",
    "print(comparisons.datapoints)\n",
    "gp.update(comparisons)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the path the user likes best and has the maximum a posteriori (MAP) estimate\n",
    "u_v, _ = gp.get_predictive_params(val_vector_p, True) #The maximum a posteriori (MAP) estimate is the mean from gaussian_process.get_predictive_params()\n",
    "print(u_v)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p_star_index = np.argmax(u_v)\n",
    "p_star_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p_star = P[p_star_index]\n",
    "p_star"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_domain = np.array(C) #set of candidate targets\n",
    "print(input_domain)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialise the acquisition function\n",
    "acq_fun = acquisition_function.DiscreteAcquirer(input_domain=input_domain, query_type='ranking', seed=123, acquisition_type='expected improvement')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: The next code cells are in a while-loop\n",
    "# while C:\n",
    "#     expected_improvement = acquisition_function.get_expected_improvement(input_domain, gp, acq_fun.history)\n",
    "#     t_index = np.argmax(expected_improvement)\n",
    "#     t = C[t_index]\n",
    "#     C.remove(t)\n",
    "#\n",
    "# # t_index\n",
    "# print(t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "expected_improvement = acquisition_function.get_expected_improvement(input_domain, gp, acq_fun.history)\n",
    "print(expected_improvement)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t_index = np.argmax(expected_improvement)\n",
    "t_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = input_domain[t_index]\n",
    "print(t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(C[0][t])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove t from C\n",
    "# C = np.delete(C, np.where(np.all(C == t)))\n",
    "indices = [i for i, x in enumerate(C) if np.all(x == t)]\n",
    "for index in sorted(indices, reverse=True):\n",
    "    del C[index]\n",
    "print(C)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "v_n = {}\n",
    "threshold=1e-8\n",
    "max_iter=1000\n",
    "obj = 'length'\n",
    "next_node = {}\n",
    "edge_cost = []\n",
    "\n",
    "for n in G:\n",
    "    v_n[n] = np.inf\n",
    "    if n == T:\n",
    "        v_n[n] = 0\n",
    "\n",
    "for i in range(max_iter): #or until convergence\n",
    "\n",
    "    converged = True\n",
    "\n",
    "    # print(\"iteration:\", i)\n",
    "    for e in G.edges(data=True):\n",
    "        n1, n2 = e[0], e[1]\n",
    "        # if v_n[n1] != np.inf or v_n[n2] != np.inf:\n",
    "        #     print(e[0], e[1], \":\", v_n[n1], v_n[n2])\n",
    "\n",
    "        # print(n2 in G)\n",
    "        cost = e[2][obj]\n",
    "        edge_cost.append(cost)\n",
    "        # print(cost)\n",
    "\n",
    "            # print(e)\n",
    "        result1 = min(cost + v_n[n2], v_n[n1])\n",
    "        result2 = min(cost + v_n[n1], v_n[n2])\n",
    "            # print(\"result1:\", result1, \"sum1:\", cost + v_n[n2])\n",
    "            # print(\"result2\", result2, \"sum2:\", cost + v_n[n1])\n",
    "        if v_n[n1] != result1 or v_n[n2] != result2:\n",
    "            converged = False\n",
    "\n",
    "        if v_n[n1] != result1:\n",
    "            next_node[n1] = n2\n",
    "\n",
    "        if v_n[n2] != result2:\n",
    "            next_node[n2] = n1\n",
    "\n",
    "        v_n[n1] = result1\n",
    "        v_n[n2] = result2\n",
    "\n",
    "        # if n1 == T or n2==T:\n",
    "        #     # print(result1, result2)\n",
    "        #     print(v_n[n1], v_n[n2])\n",
    "\n",
    "\n",
    "    # converged = all(n in v_n_copy and abs(v_n[n] - v_n_copy[n]) < threshold for n in v_n)\n",
    "    if converged:\n",
    "        break\n",
    "\n",
    "\n",
    "        # if v_n[n1] != np.inf:\n",
    "        #     print(n1,v_n[n1])\n",
    "        # if v_n[n2] != np.inf:\n",
    "        #     print(n2,v_n[n2])\n",
    "\n",
    "print(v_n)\n",
    "# print(next_node)\n",
    "\n",
    "# S = (122245.37633330293, 486126.8581684635)\n",
    "# T = (122246.77932030056, 486223.5791244763)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import single_vi_iter\n",
    "# lower_bounds = []\n",
    "# next_nodes = []\n",
    "\n",
    "lower_length = single_vi_iter.single_value_iter(G, T, 'length')\n",
    "lower_crossing = single_vi_iter.single_value_iter(G, T, 'crossing')\n",
    "# next_nodes.append(np.array([next_node_length, next_node_crossing]))\n",
    "# print(\"NEXT:\", next_node_crossing)\n",
    "\n",
    "# lower_bounds.append(np.array([lower_length, lower_crossing]))\n",
    "# lower_bounds\n",
    "\n",
    "#crossing: (122246.10058319086, 486224.512771215): (122246.77932030056, 486223.5791244763)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 0  # track iterations of the algorithm\n",
    "max_iter = 1000\n",
    "\n",
    "cost_history = np.array([0,0]) # What we've already seen\n",
    "\n",
    "stack = [(S, cost_history, [S])]  # (starting node, cost so far, path), where cost_history is from previous_state to S, path is from S to current_state (i.e., S)\n",
    "\n",
    "# r1 = 0 # Result from objective length\n",
    "# r2 = 0 # Result from objective crossing\n",
    "result = [] # The new lower bound\n",
    "\n",
    "# min_distance = np.inf\n",
    "# closest_node = None\n",
    "# val_vector_path = []\n",
    "\n",
    "\n",
    "while stack:\n",
    "    current_node, current_cost, path = stack.pop()  # current_cost=total cost up to the current_node\n",
    "\n",
    "    if current_node == T:\n",
    "        U = current_cost\n",
    "        print(f\"Path {path} with cost {current_cost}\")\n",
    "\n",
    "    neighbor_list = []\n",
    "\n",
    "    for neighbor in G.neighbors(current_node):\n",
    "        edge = G[current_node][neighbor]\n",
    "        list = [v for k, v in edge.items()]\n",
    "        # print(edge)\n",
    "        cost = np.array([list[0]['length'], list[0]['crossing']])\n",
    "\n",
    "        result = current_cost + cost + np.array([lower_length[neighbor], lower_crossing[neighbor]]) # This is the new lower bound\n",
    "        # print(result)\n",
    "\n",
    "        # Pruning paths that won't be Pareto-better compared to the current upper bound\n",
    "        if np.any(np.greater(result, U)): # If it's outside of target region, ignore it\n",
    "            continue\n",
    "\n",
    "        distance = np.sum(np.abs(t - result)) # Manhattan distance\n",
    "\n",
    "        neighbor_list.append((neighbor, distance, (current_cost + cost)))\n",
    "\n",
    "\n",
    "    neighbor_list.sort(key=lambda x:x[1], reverse=True)\n",
    "    for n in neighbor_list:\n",
    "        path_copy = path.copy()\n",
    "        path_copy.append(n[0])\n",
    "        stack.append((n[0], n[2], path_copy))\n",
    "\n",
    "    i += 1\n",
    "    if max_iter is not None and i >= max_iter:\n",
    "        print(\"The algorithm has reached the given maximum iterations, but has found no solution.\")\n",
    "        break\n",
    "\n",
    "# S = (122245.37633330293, 486126.8581684635)\n",
    "# T = (122246.77932030056, 486223.5791244763)\n",
    "# t = [165.21   2.  ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p_s = path\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import dfs_lower\n",
    "p_t, val_p_t, new_U = dfs_lower.dfs_lower(G, S, T, t, U, max_iter=None)  # Change max_iter when doing experiments\n",
    "# U = new_U\n",
    "print(f\"val_p_t={val_p_t} and U:{U}\")\n",
    "# If v^p_t improves in the target region\n",
    "if np.any(val_p_t < U):\n",
    "        P = P.append(p_t)\n",
    "\n",
    "        # Compare p^t to p^∗ and add comparison to the GP ▷ User ranking, i.e., is the new path preferred to the current, maximum one?\n",
    "        compare_ps_pstar.append(np.array([val_p_t, val_vector_p_star]))\n",
    "        ranking_new_paths = user_preference.get_preference(compare_ps_pstar, add_noise=add_noise)\n",
    "        print(f\"Ground-truth utility for p*: {ranking_new_paths}\")\n",
    "\n",
    "            # Add the comparisons to the GP\n",
    "        comparisons.add_single_comparison(compare_ps_pstar[np.argmax(ranking_new_paths)], compare_ps_pstar[np.argmin(ranking_new_paths)])\n",
    "        gp.update(comparisons)\n",
    "\n",
    "            # if u(v^{p^t}) > u(v^{p^*}) then\n",
    "        u_v_p_t, _ = gp.get_predictive_params(val_p_t, True)  # The maximum a posteriori (MAP) estimate is the mean from gaussian_process.get_predictive_params()\n",
    "        u_v_p_star, _ = gp.get_predictive_params(val_vector_p_star, True)\n",
    "\n",
    "        if u_v_p_t > u_v_p_star:\n",
    "                # p^∗ ← p^t\n",
    "            p_star = p_t\n",
    "\n",
    "            # Compute new candidate targets based on v^{p^t} and add to C\n",
    "        new_C = [min(val_p_t[0][0], val_p_t[1][0]), min(val_p_t[0][1], val_p_t[1][1])]\n",
    "        C = np.append(new_C)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
